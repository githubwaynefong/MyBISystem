遇见的问题：（除了第9个问题，其余都解决了；还有一些零散的小问题，没有记录）
	1.项目运行执行python脚本时，访问不到引入的自定义python文件
		需要设置PYTHONPATH环境变量，并且执行时在项目根目录下
		如：hadoop@master MyBI]$ python com/cal/driver.py
	2.python中拼写hive命令时，执行语句用双引号，外部传入的参数需要带单引号，如："where dt = '" + param + "'"
	3.建表时多了各字段，导致读入的MapReduce输出数据列发生偏移
	4.IpParser类调用的本地文件"qqwry.dat"在MapReduce作业中无法读取到。
		原因是：MR作业为分布式作业，在不同节点进行，计算结点就没有作业提交节点的本地文件
		        所以需要将文件上传到HDFS路径下，加入Job的分布式缓存中，缓存到计算结点本地。
	5.MapReduce作业中使用到的字符串分隔符容易弄乱，导致解析失败
	6.正则匹配字符串中的符号需要用转义
	7.System.out.println();只能在Mapper/Reducer中输出，import进入的自定义类中的输出不会输出
		输出在如下路径：
		http://192.168.190.201:8042/logs/userlogs/application_1558756836820_0011/container_1558756836820_0011_01_000002/stdout
	8.Hive中执行join连接时，报错：提示笛卡儿积不安全（避免笛卡儿积操作占用大量计算资源，而使集群瘫痪）：
		如：FAILED: SemanticException Cartesian products are disabled for safety reasons.
			If you know what you are doing, please set hive.strict.checks.cartesian.product to false 
			and that hive.mapred.mode is not set to 'strict' to proceed. 
			Note that if you may get errors or incorrect results if you make a mistake while using some of the unsafe features.
		解决方法：
			设置：hive> set hive.mapred.mode=nonstrict
	9.聚类模块的mahout无法运行，未能解决，报错：
		错误：java.lang.ClassNotFoundException: Class org.apache.mahout.math.VectorWritable not found
	  1.试过 hadoop jar .... -libjars 不行；
	  2.试过 全部连lib包一起导出，也不行；
	  3.发现 mahout-core-0.7-job.jar和mahout-core-0.7.jar两个包中均有org.apache.mahout.math.VectorWritable
		  是不是冲突了，试过只导入一个，也不行。
	10.Sqoop语句中不能包含/n/t换行符，否则编译执行时会转换成奇怪的字符导致无法执行
		错误提示：
			sh: -c:行44: 未预期的符号 `;' 附近有语法错误
			sh: -c:行51: `; } 2>&1'
		解决：
			xml中的Sqoop语句继续是回车换行，方便阅读，只是在xml被解析成字符串时，会将这些符号替换成单个空格
			这样，即不影响阅读，也不影响执行
	
-----------------------------------------------------------------------------------
--------------------------《导入导出模块》-----------------------------------------
---------------从业务系统导出数据到Hive数仓，经过清洗分析后再导出到业务系统--------
-----------------------------------------------------------------------------------
模块一：导入：使用Sqoop工具全量导入业务数据到Hive数仓
	1.自建业务数据放入MySQL（数据库名：booksys）
		业务表创建及数据初始在项目路径："/MyBI/tables_init/tables_init_in_mysql.sql"
	2.执行导入模块：2019-05-18之前的所有数据，这里只导入一张表orders（订单数据表）
		$ cd ~/MyBI
		$ python com/cal/import.py '2019-05-18'
	3.导入成功后，从Hive中查看
		hive> select * from booksys.orders where dt='2019-05-18';

模块二：导出：使用Sqoop工具导出分析数据到业务数据库MySQL中
	1.在MySQL中建导出数据接收表
		脚本路径：/MyBI/tables_init/tables_init_in_mysql_from hive_export.sql
	2.执行导出模块：2019-05-18这天的数据，只导出orders表
		$ cd ~/MyBI
		$ python com/cal/export.py '2019-05-18'
	3.导出成功后，从MySQL中查看

-----------------------------------------------------------------------------------
--------------------------《数据清洗/分析模块》------------------------------------
---------------------对导入Hive数仓的数据清洗，并分析------------------------------
-----------------------------------------------------------------------------------
模块一：数据分析模块
	1.只做简单的查询
		$ cd ~/MyBI
		$ python com/cal/exe_hql.py 'analysis' '2019-05-18'

模块二：数据清洗模块
	  重点做数据去重（增量导入时，修改过的记录将再次被导入，产生重复ID的记录，需要取最新数据）
	1.修改"/MyBI/conf/Import.xml"中的 task.type = 'add'，即增量导入
	2.执行业务数据库MySQL中数据更新操作
		脚本路径：/MyBI/tables_init/update_data_in_mysql.sql（数据更新日期为 2019-05-19）
	3.执行增量导入模块：
		$ cd ~/MyBI
		$ python com/cal/import.py '2019-05-19'
		此时，从全量到增量的两天数据已经进入Hive数仓的orders表中
	4.执行清洗（去重）模块：
		（两种方法实现：1.all与add合并，按ID进行row_number()排序； 
					    2.all对add取查集，后与add并，达到去重
						最后利用动态分区，重新写入Hive的orders表中）
		$ cd ~/MyBI
		$ python com/cal/exe_hql.py 'etl_db' '2019-05-19'
	5.查看结果：
		hive> select * from booksys.orders where dt<='2019-05-19';
		会发现：成功去除分区dt=2019-05-18中的三条在2019-05-19已更新的数据；
		效果是：所有数据单号ID唯一，且为最新状态（截止2019-05-19这天）

-----------------------------------------------------------------------------------
-----------------------《点击流日志清洗/分析模块》---------------------------------
---------------分析购书网站服务器日志，并计算网站购书转换率------------------------
-----------------------------------------------------------------------------------
模块一：点击流日志清洗模块
	流程：1.由日志服务器将日志上传到到HDFS下的'/tmp/apache_log/'
		  2.MapReduce读取上述路径下的日志数据进行解析清洗
		  3.MapReduce将作业结果输出至Hive表clickstream_log的HDFS存储路径下
		  4.上传作业结果至Hive中（其实数据仍然在clickstream_log目录下，只是向Hive注册）

	1.数据源准备
		由日志服务器将日志上传到到HDFS下的'/tmp/apache_log/'（此处用手动上传模拟）
	1.1.创建HDFS接收日志的目录
		$ hadoop fs -mkdir /tmp/apache_log/2019-05-22
		**（二次测试用）如果目录下已经有数据，先删除
			$ hadoop fs -rm -r /tmp/apache_log/2019-05-22/*
	1.2.向目录中上传日志数据
		$ hadoop fs -put ~/MyBI/logs/logs_click /tmp/apache_log/2019-05-22
	1.3.向HDFS分布式文件系统中上传IP数据库文件"hdfs://master:9000/user/hadoop/cz88/qqwry.dat"
		$ hadoop fs -mkdir /user/hadoop/cz88
		$ hadoop fs -put ~/MyBI/lib/cz88/qqwry.dat /user/hadoop/cz88/
		
	2.MapReduce作业输出的Hive表初始化
		输出路径为：Hive中的clickstream_log表的HDFS路径
		如：/user/hive/warehouse/clickstream_log/
	2.1.在Hive中创建点击流日志清洗结果表clickstream_log，用于存放MR输出
		$ cd ~/MyBI/tables_init
		$ hive -f 'tables_init_in_hive.hql'
		**（二次测试用）删除clickstream_log表数据
			1.进入Hive，使用默认的default数据库
			$ hive
			2.删除表分区数据
			hive> alter table clickstream_log drop partition(dt='2019-05-22');
			hive> exit;
	
	3.执行点击流日志清洗模块
		$ cd ~/MyBI
		$ python com/cal/etl_clickstream.py '2019-05-22'
		<执行完MapReduce作业不报错，即执行成功>
		
		注：用到的日志清洗Jar包：/MyBI/lib/clickstream_etl.jar
			对应Jar包的源码位置：
			/MyBI_JUtils/com.etl.mapreduce
						/com.etl.utls
						/cz88/qqwry.dat

	4.进入Hive查看日志解析结果数据
		$ hive
		hive> select * from clickstream_log where dt='2019-05-22';
		<将查询出16条数据，与logs_click文件中的数据一致，只是被结构化存储了>
		
模块二：购书转换率分析模块
	流程：1.从clickstream_log中抽取MR作业需要的数据到conversion_input表中
		  2.执行MapReduce作业，从conversion_input表的HDFS路径下读取数据
		  3.MR执行对网站购书漏斗模型的URL计数
				*漏斗模型的三个URL：（各个URL的点击数逐层递减）
					|							 /
					| 图书展示页面books.do    	/	100次
					| 购书订单页面orders.do    /	50次
					| 支付完成页面pay.do	  /		25次
					|						 /
		  4.将计数结果输出到HDFS下的/user/tmp/conversion中
		  5.将MR作业输出结果上传至Hive的转换率中间结果表conversion_middle_result表中
		  6.对conversion_middle_result表数据统计，统计结果输出至Hive的conversion_result表中
		  
	1.对涉及到的Hive表初始化
		此步已在上一模块中做过，不许再执行，$ hive -f 'tables_init_in_hive.hql'
	2.设置MR作业输出路径：/user/tmp/conversion
		$ hadoop fs -mkdir /user/tmp
		$ hadoop fs -mkdir /user/tmp/conversion
	3.执行转换率分析模块
		$ cd ~/MyBI
		$ python com/cal/conversion.py '2019-05-22' '2019-05-23'
	4.成功执行后，查看conversion_result表
		hive> select process, process_count, uuid_count from conversion_result where dt = '2019-05-22-2019-05-23';
					1			5				3
					2			5				3
					3			3				2
		即：步骤1（books.do），有效点击次数5次，操作人数3人
			步骤2（orders.do），有效点击次数5次，操作人数3人
			步骤3（pay.do），有效点击次数3次，操作人数2人
		转换率：(1->2)：5/5=100%
			    (2->3): 3/5=60%
				
-----------------------------------------------------------------------------------
--------------------------《购书用户聚类模块》-------------------------------------
------------------------------（待完善）-------------------------------------------
-----------------------------------------------------------------------------------
	1.对涉及到的Hive表初始化
		此步已在上一模块中做过，不许再执行，$ hive -f 'tables_init_in_hive.hql'
	2.初始化cluster_input数据，此处从简，直接造了点数据（应从user_dimension中获取，而user_dimension应从clickstream_log和orders中获取）
		hive> load data local inpath '/home/hadoop/MyBI/tables_init/cluster_input_data' overwrite into table cluster_input;
		数据特征维度已经通过相关性计算：
			脚本路径：/MyBI/tables_init/cluster_data_design_in_mysql.sql
	3.创建聚类输出HDFS路径'/user/hadoop/cluster_output'
		$ hadoop fs -mkdir /user/hadoop/cluster_output
	4.执行聚类模块（到此为止，没有运行起来，未解决错误：java.lang.ClassNotFoundException: Class org.apache.mahout.math.VectorWritable not found）
		$ cd ~/MyBI
		$ python com/cal/cluster_user.py '2019-05-22' '2019-05-23'
		$ hadoop jar usercluster.jar com.mahout.UserCluster /user/hadoop/cluster_output /user/hive/warehouse/cluster_input 100 10 0.5 10
		